{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qspqbJOVOpN-"
      },
      "source": [
        "## Equivaraint TD3 Actor and QNetwork Test (Pytorch Implementation)\n",
        "### Symmetrzier Package\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from symmetrizer.nn.modules import BasisLinear\n",
        "from symmetrizer.ops import GroupRepresentations\n",
        "from symmetrizer.groups import MatrixRepresentation\n",
        "import numpy as np\n",
        "\n",
        "class InvariantQNetwork(nn.Module):\n",
        "    def __init__(self, repr_in, repr_out, hidden_sizes, basis=\"equivariant\", gain_type=\"xavier\"):\n",
        "        super().__init__()\n",
        "        self.fc1 = BasisLinear(1, hidden_sizes, repr_in, basis=basis, gain_type=gain_type, n_samples=4096)\n",
        "        self.fc2 = BasisLinear(hidden_sizes, hidden_sizes, repr_out, basis=basis, gain_type=gain_type,  n_samples=4096)\n",
        "        self.fc3 = BasisLinear(hidden_sizes, 1, repr_out, basis=basis, gain_type=gain_type,  n_samples=4096)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x, a = x.unsqueeze(1), a.unsqueeze(1)\n",
        "        x = torch.cat([x, a], 2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze(1)\n",
        "\n",
        "\n",
        "class EquiActor(nn.Module):\n",
        "    def __init__(self, repr_in, repr_out,hidden_repr, hidden_size, basis=\"equivariant\", gain_type=\"xavier\"):\n",
        "        super().__init__()\n",
        "        self.fc1 = BasisLinear(1, hidden_size, group=repr_in, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
        "        self.fc2 = BasisLinear(hidden_size, hidden_size, group=hidden_repr, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
        "        self.fc2 = BasisLinear(hidden_size, hidden_size, group=hidden_repr, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
        "        self.fc_mu = BasisLinear(hidden_size, 1, group=repr_out, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
        "        \n",
        "        # action rescaling\n",
        "        # self.register_buffer(\"action_scale\", torch.tensor((env.single_action_space.high - env.single_action_space.low) / 2.0, dtype=torch.float64))\n",
        "        # self.register_buffer(\"action_bias\", torch.tensor((env.single_action_space.high + env.single_action_space.low) / 2.0, dtype=torch.float64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc_mu(x))\n",
        "        #x = x * self.action_scale + self.action_bias\n",
        "        x = x.squeeze(1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "ch = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "representations = [torch.FloatTensor(np.eye(4)), torch.FloatTensor(-1 * np.eye(4))]\n",
        "in_group = GroupRepresentations(representations, \"StateGroupRepr\")\n",
        "\n",
        "representations = [torch.FloatTensor(np.eye(8)), torch.FloatTensor(-1 * np.eye(8))]\n",
        "h_group = GroupRepresentations(representations, \"HiddenGroupRepr\")\n",
        "\n",
        "representations = [torch.FloatTensor(np.eye(1)), torch.FloatTensor(-1 * np.eye(1))]\n",
        "out_group = GroupRepresentations(representations, \"ActionGroupRepr\")\n",
        "\n",
        "repr_in = MatrixRepresentation(in_group, h_group)\n",
        "repr_hidden = MatrixRepresentation(h_group, h_group)\n",
        "repr_out = MatrixRepresentation(h_group, out_group)\n",
        "actor = EquiActor(repr_in, repr_out, repr_hidden , ch).to(device)\n",
        "\n",
        "\n",
        "representations = [torch.FloatTensor(np.eye(5)), torch.FloatTensor(-1 * np.eye(5))]\n",
        "in_group = GroupRepresentations(representations, \"StateGroupRepr\")\n",
        "\n",
        "representations = [torch.FloatTensor(np.eye(10)), torch.FloatTensor(-1 * np.eye(10))]\n",
        "in_ghoup = GroupRepresentations(representations, \"HiddenGroupRepr\")\n",
        "representations = [torch.FloatTensor(np.eye(1)), torch.FloatTensor(-1 * np.eye(1))]\n",
        "out_group = GroupRepresentations(representations, \"ActionGroupRepr\")\n",
        "representations = [torch.FloatTensor(np.eye(1)), torch.FloatTensor(np.eye(1))]\n",
        "out_group_q =  GroupRepresentations(representations, \"InvariantGroupRepr\")\n",
        "\n",
        "\n",
        "repr_in_q = MatrixRepresentation(in_group, out_group)\n",
        "repr_out_q = MatrixRepresentation(out_group_q, out_group_q)\n",
        "\n",
        "qf = InvariantQNetwork( repr_in_q, repr_out_q, ch).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def actor_equivariance_mae(network, obs: torch.Tensor, repr_in: MatrixRepresentation, repr_out: MatrixRepresentation) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the MSE of the equivariance error for the actor network.\n",
        "    \"\"\"\n",
        "    # Move the matrices to the network's device\n",
        "    device = obs.device\n",
        "    dtype = obs.dtype\n",
        "    repr_in_matrices = [p_in.to(device, dtype) for p_in in repr_in._input_matrices]\n",
        "    repr_out_matrices = [p_out.to(device, dtype) for p_out in repr_out._output_matrices]\n",
        "\n",
        "    transformed_inputs = torch.stack([obs @ p_in for p_in in repr_in_matrices])\n",
        "    \n",
        "    def get_only_mean(x):\n",
        "        if isinstance(x, tuple):\n",
        "            return x[0]\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    y1 = torch.stack([get_only_mean(network(p_x)) for p_x in transformed_inputs])\n",
        "    y2 = torch.stack([get_only_mean(network(obs)) @ p_out  for p_out in repr_out_matrices])\n",
        "\n",
        "    return (y1.abs() - y2.abs()).abs().mean().item()\n",
        "\n",
        "def q_equivariance_mae(network, obs: torch.Tensor, actions: torch.Tensor, repr_in_q: MatrixRepresentation) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the MSE of the equivariance error for the Q-network.\n",
        "    \"\"\"\n",
        "    device = obs.device\n",
        "    dtype = obs.dtype\n",
        "    obs_actions = torch.cat([obs, actions], dim=-1)\n",
        "    # Move the matrices to the network's device\n",
        "    repr_in_q_matrices = [p_in.to(device, dtype) for p_in in repr_in_q._input_matrices]\n",
        "    \n",
        "    transformed_inputs = torch.stack([obs_actions @ p_in for p_in in repr_in_q_matrices])\n",
        "    y1 = torch.stack([network(p_obs_actions[:, :obs.size(-1)], p_obs_actions[:, obs.size(-1):]) for p_obs_actions in transformed_inputs])\n",
        "    y2 = network(obs, actions).unsqueeze(0).expand_as(y1)\n",
        "\n",
        "    return (y1.abs() - y2.abs()).abs().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actor equivariance error: 4.17e-03\n",
            "Q-network equivariance error: 1.42e-01\n"
          ]
        }
      ],
      "source": [
        "obs = torch.randn(50, 4, dtype=torch.float64) \n",
        "a = torch.randn(50, 1)\n",
        "err_a = actor_equivariance_mae(actor, obs, repr_in, repr_out)\n",
        "print(f\"Actor equivariance error: {err_a:.2e}\")\n",
        "err_q = q_equivariance_mae(qf, obs, a, repr_in_q)\n",
        "print(f\"Q-network equivariance error: {err_q:.2e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
