{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qspqbJOVOpN-"
   },
   "source": [
    "## Equivaraint TD3 Actor and QNetwork Test (Pytorch Implementation)\n",
    "### Symmetrzier Package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from symmetrizer.nn.modules import BasisLinear\n",
    "from symmetrizer.ops import GroupRepresentations\n",
    "from symmetrizer.groups import MatrixRepresentation\n",
    "from symmetrizer.groups import P4\n",
    "import numpy as np\n",
    "\n",
    "class InvariantQNetwork(nn.Module):\n",
    "    def __init__(self, repr_in, repr_out, hidden_sizes, basis=\"equivariant\", gain_type=\"xavier\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = BasisLinear(1, hidden_sizes, repr_in, basis=basis, gain_type=gain_type, n_samples=4096)\n",
    "        self.fc2 = BasisLinear(hidden_sizes, hidden_sizes, repr_out, basis=basis, gain_type=gain_type,  n_samples=4096)\n",
    "        self.mu = BasisLinear(1, 1, repr_out, basis=basis, gain_type=gain_type,  n_samples=4096)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x, a = x.unsqueeze(1), a.unsqueeze(1)\n",
    "        x = torch.cat([x, a], 2)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.mu(x).squeeze(1)\n",
    "\n",
    "\n",
    "class EquiActor(nn.Module):\n",
    "    def __init__(self, repr_in, repr_out, hidden_size, basis=\"equivariant\", gain_type=\"xavier\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = BasisLinear(1, hidden_size, group=repr_in, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
    "        self.fc2 = BasisLinear(hidden_size, hidden_size, group=repr_in, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
    "        self.fc_mu = BasisLinear(hidden_size, 1, group=repr_out, basis=basis, gain_type=gain_type, bias_init=False,  n_samples=4096)\n",
    "        \n",
    "        # action rescaling\n",
    "        # self.register_buffer(\"action_scale\", torch.tensor((env.single_action_space.high - env.single_action_space.low) / 2.0, dtype=torch.float64))\n",
    "        # self.register_buffer(\"action_bias\", torch.tensor((env.single_action_space.high + env.single_action_space.low) / 2.0, dtype=torch.float64))\n",
    "                # Print weights to observe structure\n",
    "        print(\"Weights of fc1:\")\n",
    "        print(self.fc1.basis)\n",
    "        \n",
    "        print(\"Weights of fc2:\")\n",
    "        print(self.fc2.basis)\n",
    "        print(\"Weights of fc_mu:\")\n",
    "        print(self.fc_mu.basis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        print(\"frist \\n\",x)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        print(\"second \\n\",x)\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc_mu(x)\n",
    "        print(\"third \\n\",x)\n",
    "        #x = x * self.action_scale + self.action_bias\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def display_heatmap(tensor, title=\"Heatmap\"):\n",
    "    # Convert tensor to numpy if it's a PyTorch tensor\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        tensor = tensor.cpu().detach().numpy()\n",
    "    \n",
    "    # Ensure the tensor is 2D for a heatmap; reshape if necessary\n",
    "    # This example flattens all but the first two dimensions\n",
    "    if tensor.ndim > 2:\n",
    "        tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(tensor, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "ch = 4\n",
    "I = np.eye(4)\n",
    "rotations = [\n",
    "    torch.FloatTensor(I),                    # 0째 rotation (identity matrix)\n",
    "    torch.FloatTensor(np.roll(I, 1, axis=0)), # 90째 rotation (1st cyclic permutation)\n",
    "    torch.FloatTensor(np.roll(I, 2, axis=0)), # 180째 rotation (2nd cyclic permutation)\n",
    "    torch.FloatTensor(np.roll(I, 3, axis=0))  # 270째 rotation (3rd cyclic permutation)\n",
    "]\n",
    "\n",
    "print(rotations[3])\n",
    "np.rot90(np.eye(5), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of fc1:\n",
      "tensor([[[[[ 0.7855, -0.0876, -0.5077,  0.3428]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.3761,  0.4995, -0.7666, -0.1460]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.2400,  0.3432,  0.1716,  0.8917]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.4289,  0.7906,  0.3537, -0.2569]]]]], dtype=torch.float64)\n",
      "Weights of fc2:\n",
      "tensor([[[[[ 0.8317, -0.3815, -0.0997, -0.3909]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0282,  0.5402, -0.7407, -0.3983]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.4165, -0.7479, -0.5164, -0.0246]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.3661, -0.0575,  0.4180, -0.8294]]]]], dtype=torch.float64)\n",
      "Weights of fc_mu:\n",
      "tensor([[[[[1.]]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "representations = [torch.FloatTensor(np.eye(4)), torch.FloatTensor(-1 * np.eye(4))]\n",
    "in_group = GroupRepresentations(representations, \"StateGroupRepr\")\n",
    "\n",
    "representations = [torch.FloatTensor(np.eye(1)), torch.FloatTensor(-1 * np.eye(1))]\n",
    "out_group = GroupRepresentations(representations, \"ActionGroupRepr\")\n",
    "\n",
    "repr_in = MatrixRepresentation(in_group, out_group)\n",
    "repr_out = MatrixRepresentation(out_group, out_group)\n",
    "actor = EquiActor(repr_in, repr_out , ch).to(device)\n",
    "\n",
    "\n",
    "representations = [torch.FloatTensor(np.eye(5)), torch.FloatTensor(-1 * np.eye(5))]\n",
    "in_group = GroupRepresentations(representations, \"StateGroupRepr\")\n",
    "\n",
    "representations = [torch.FloatTensor(np.eye(5)), torch.FloatTensor( [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0]])]\n",
    "out_group = GroupRepresentations(representations, \"ActionGroupRepr\")\n",
    "representations = [torch.FloatTensor(np.eye(1)), torch.FloatTensor(np.eye(1))]\n",
    "out_group_q =  GroupRepresentations(representations, \"InvariantGroupRepr\")\n",
    "\n",
    "\n",
    "repr_in_q = MatrixRepresentation(in_group, out_group)\n",
    "repr_out_q = MatrixRepresentation(out_group, out_group)\n",
    "repr_out_qf = MatrixRepresentation(out_group, out_group_q)\n",
    "\n",
    "\n",
    "# qf = InvariantQNetwork( repr_in_q, repr_out_q, repr_out_qf, ch).to(device)\n",
    "# print(actor.fc1.basis)\n",
    "# print(actor.fc1.coeffs)\n",
    "# display_heatmap(actor.fc2.basis*actor.fc2.coeffs, title=\"Heatmap of fc1 Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def actor_equivariance_mae(network, obs: torch.Tensor, repr_in: MatrixRepresentation, repr_out: MatrixRepresentation) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the MSE of the equivariance error for the actor network.\n",
    "    \"\"\"\n",
    "    # Move the matrices to the network's device\n",
    "    device = obs.device\n",
    "    dtype = obs.dtype\n",
    "    repr_in_matrices = [p_in.to(device, dtype) for p_in in repr_in._input_matrices]\n",
    "    repr_out_matrices = [p_out.to(device, dtype) for p_out in repr_out._output_matrices]\n",
    "\n",
    "    transformed_inputs = torch.stack([obs @ p_in for p_in in repr_in_matrices])\n",
    "    \n",
    "    def get_only_mean(x):\n",
    "        if isinstance(x, tuple):\n",
    "            return x[0]\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    y1 = torch.stack([get_only_mean(network(p_x)) for p_x in transformed_inputs])\n",
    "    y2 = torch.stack([get_only_mean(network(obs)) @ p_out  for p_out in repr_out_matrices])\n",
    "\n",
    "    return (y1.abs() - y2.abs()).abs().mean().item()\n",
    "\n",
    "def q_equivariance_mae(network, obs: torch.Tensor, actions: torch.Tensor, repr_in_q: MatrixRepresentation) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the MSE of the equivariance error for the Q-network.\n",
    "    \"\"\"\n",
    "    device = obs.device\n",
    "    dtype = obs.dtype\n",
    "    obs_actions = torch.cat([obs, actions], dim=-1)\n",
    "    # Move the matrices to the network's device\n",
    "    repr_in_q_matrices = [p_in.to(device, dtype) for p_in in repr_in_q._input_matrices]\n",
    "    \n",
    "    transformed_inputs = torch.stack([obs_actions @ p_in for p_in in repr_in_q_matrices])\n",
    "    y1 = torch.stack([network(p_obs_actions[:, :obs.size(-1)], p_obs_actions[:, obs.size(-1):]) for p_obs_actions in transformed_inputs])\n",
    "    y2 = network(obs, actions).unsqueeze(0).expand_as(y1)\n",
    "\n",
    "    return (y1 - y2).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9667, -0.4602,  0.1567, -0.1084]], dtype=torch.float64)\n",
      "frist \n",
      " tensor([[[ 0.9667, -0.4602,  0.1567, -0.1084]]], dtype=torch.float64)\n",
      "second \n",
      " tensor([[[-0.0555],\n",
      "         [-0.4804],\n",
      "         [-0.1122],\n",
      "         [-0.2113]]], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "third \n",
      " tensor([[[-0.6703]]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "frist \n",
      " tensor([[[-0.9667,  0.4602, -0.1567,  0.1084]]], dtype=torch.float64)\n",
      "second \n",
      " tensor([[[0.0555],\n",
      "         [0.4804],\n",
      "         [0.1122],\n",
      "         [0.2113]]], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "third \n",
      " tensor([[[0.6703]]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "frist \n",
      " tensor([[[ 0.9667, -0.4602,  0.1567, -0.1084]]], dtype=torch.float64)\n",
      "second \n",
      " tensor([[[-0.0555],\n",
      "         [-0.4804],\n",
      "         [-0.1122],\n",
      "         [-0.2113]]], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "third \n",
      " tensor([[[-0.6703]]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "frist \n",
      " tensor([[[ 0.9667, -0.4602,  0.1567, -0.1084]]], dtype=torch.float64)\n",
      "second \n",
      " tensor([[[-0.0555],\n",
      "         [-0.4804],\n",
      "         [-0.1122],\n",
      "         [-0.2113]]], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "third \n",
      " tensor([[[-0.6703]]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Actor equivariance error: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "obs = torch.randn(1, 4, dtype=torch.float64) \n",
    "print(obs)\n",
    "a = torch.randn(1, 1)\n",
    "err_a = actor_equivariance_mae(actor, obs, repr_in, repr_in)\n",
    "print(f\"Actor equivariance error: {err_a:.2e}\")\n",
    "#err_q = q_equivariance_mae(qf, obs, a, repr_in_q)\n",
    "#print(f\"Q-network equivariance error: {err_q:.2e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cleanrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
