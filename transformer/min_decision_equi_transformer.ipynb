{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2Nk5Gp7hUGA"
   },
   "source": [
    "# import libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "q4xiijmBixUm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from symmetrizer.nn.modules import BasisLinear\n",
    "from symmetrizer.ops import GroupRepresentations\n",
    "from symmetrizer.groups import MatrixRepresentation\n",
    "from utils.symmetrizer_utils import create_inverted_pendulum_actor_representations, create_inverted_pendulum_qfunction_representations, actor_equivariance_mae, q_equivariance_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_emlp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the BasisLinear layer for testing\n",
    "# state_dim = 4  # Dimensionality of the input state\n",
    "# h_dim = 128     # Dimensionality of the output representation\n",
    "# # Test input: batch of states\n",
    "# batch_size = 64  # Example batch size\n",
    "# seq_len = 20    # Sequence length (context length)\n",
    "# in_action_embedding = [\n",
    "# torch.DoubleTensor(np.eye(4)), \n",
    "# torch.DoubleTensor(-1 * np.eye(4))\n",
    "# ]\n",
    "# in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "\n",
    "# out_action_embedding =  [\n",
    "#     torch.DoubleTensor(np.eye(seq_len*2)), \n",
    "#     torch.DoubleTensor(-1 * np.eye(seq_len*2))\n",
    "# ]\n",
    "# out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "# repr_inter = MatrixRepresentation(in_group, out_group)\n",
    "\n",
    "# basis_layer = BasisLinear(\n",
    "#     channels_in=1,  # Input dimensionality\n",
    "#     channels_out=h_dim,     # Output dimensionality\n",
    "#     group=repr_inter,          # Group representation for equivariance\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make(\"InvertedPendulum-v4\")\n",
    "\n",
    "# # Reset the environment and get the initial state\n",
    "# x, _ = env.reset()  # Unpack observation and metadata\n",
    "\n",
    "# # Convert the observation to a PyTorch tensor and add a batch dimension\n",
    "# t_x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)  # Shape [1, state_dim]\n",
    "# dummy_input = torch.randn(batch_size, seq_len ,state_dim)\n",
    "# print(\"Initial observation:\", dummy_input.shape)\n",
    "\n",
    "# layer = nn.Linear(state_dim, h_dim)\n",
    "# #output = basis_layer(dummy_input).permute(0, 2, 1) \n",
    "# output = layer(dummy_input)\n",
    "\n",
    "# print(\"Output shape:\", output.shape)      \n",
    "# # proint the first 10 elements of the output\n",
    "# # Print the first 10 elements of the output\n",
    "# print(\"Output:\", output[1, 1, : ].shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNJM0LG1iziA"
   },
   "source": [
    "# decision transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "MHMl_Y1SicXb"
   },
   "outputs": [],
   "source": [
    "class MaskedCausalAttention(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p, context_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.max_T = max_T\n",
    "         ## group representations\n",
    "        in_action_embedding = [\n",
    "        torch.DoubleTensor(np.eye(context_len*3)), \n",
    "        torch.DoubleTensor(-1 * np.eye(context_len*3))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "        \n",
    "        \n",
    "\n",
    "        out_action_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len*3)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len*3))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "\n",
    "        repr_inter = MatrixRepresentation(in_group, out_group)\n",
    "\n",
    "        if use_emlp:\n",
    "            self.q_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.k_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.v_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "        else:\n",
    "            self.q_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.k_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.v_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "\n",
    "        self.proj_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "        ones = torch.ones((max_T, max_T))\n",
    "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
    "\n",
    "        # register buffer makes sure mask does not get updated\n",
    "        # during backpropagation\n",
    "        self.register_buffer('mask',mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
    "\n",
    "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
    "\n",
    "        # rearrange q, k, v as (B, N, T, D)\n",
    "        if use_emlp:\n",
    "            q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
    "            k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
    "            v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        else:\n",
    "\n",
    "            q = self.q_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "            k = self.k_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "            v = self.v_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "\n",
    "        # weights (B, N, T, T)\n",
    "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
    "        # causal mask applied to weights\n",
    "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
    "        # normalize weights, all -inf -> 0 after softmax\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # attention (B, N, T, D)\n",
    "        attention = self.att_drop(normalized_weights @ v)\n",
    "\n",
    "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
    "\n",
    "        out = self.proj_drop(self.proj_net(attention))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p, context_len):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p, context_len)\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(h_dim, 4*h_dim, \n",
    "            dtype=torch.float64),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4*h_dim, h_dim,\n",
    "            dtype=torch.float64),\n",
    "                nn.Dropout(drop_p))\n",
    "        self.ln1 = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "        self.ln2 = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
    "        x = x + self.attention(x) # residual\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x) # residual\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
    "                 n_heads, drop_p, max_timestep=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.h_dim = h_dim\n",
    "        \n",
    "        ## group representations\n",
    "        in_action_embedding = [\n",
    "            torch.DoubleTensor(np.eye(act_dim)), \n",
    "            torch.DoubleTensor(-1 * np.eye(act_dim))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "        \n",
    "        \n",
    "\n",
    "        out_action_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "\n",
    "        repr_in = MatrixRepresentation(in_group, out_group)\n",
    "        repr_out = MatrixRepresentation(out_group, in_group)\n",
    "        ########################################################\n",
    "        in_state_embedding = [\n",
    "        torch.DoubleTensor(np.eye(state_dim)), \n",
    "        torch.DoubleTensor(-1 * np.eye(state_dim))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_state_embedding, \"ActionRepr\")\n",
    "        \n",
    "\n",
    "        out_state_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_state_embedding, \"ActionRepr\")\n",
    "        \n",
    "        repr_in_s = MatrixRepresentation(in_group, out_group)\n",
    "        repr_out_s = MatrixRepresentation(out_group, in_group)\n",
    "        ### transformer blocks\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p, context_len) for _ in range(n_blocks)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "\n",
    "        ### projection heads (project to embedding)\n",
    "        self.embed_ln = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
    "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
    "        if use_emlp:\n",
    "            self.embed_state = BasisLinear(1, h_dim, group=repr_in_s)\n",
    "        else:\n",
    "            self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
    "\n",
    "        # # discrete actions\n",
    "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
    "        # use_action_tanh = False # False for discrete actions\n",
    "\n",
    "        # continuous actions\n",
    "        if use_emlp:\n",
    "            self.embed_action = BasisLinear(1, h_dim, group=repr_in)\n",
    "            use_action_tanh = True # True for continuous actions\n",
    "        else:\n",
    "            self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
    "            use_action_tanh = False\n",
    "\n",
    "        ### prediction heads\n",
    "        self.predict_rtg = torch.nn.Linear(h_dim, 1, dtype=torch.float64)\n",
    "        \n",
    "        if use_emlp:\n",
    "            self.predict_state = BasisLinear(h_dim, 1, group=repr_out_s)\n",
    "            self.predict_action = nn.Sequential(\n",
    "                *([BasisLinear(h_dim, 1, group=repr_out)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "            )\n",
    "        else:\n",
    "            self.predict_state = torch.nn.Linear(h_dim, state_dim, dtype=torch.float64)\n",
    "            self.predict_action = nn.Sequential(\n",
    "                *([nn.Linear(h_dim, act_dim, dtype=torch.float64)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, timesteps, states, actions, returns_to_go):\n",
    "\n",
    "        B, T, _ = states.shape\n",
    "\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = self.embed_state(states) #+ time_embeddings\n",
    "        action_embeddings = self.embed_action(actions)# + time_embeddings\n",
    "        returns_embeddings = self.embed_rtg(returns_to_go) #+ time_embeddings\n",
    "        \n",
    "        if use_emlp:\n",
    "            state_embeddings = state_embeddings.permute(0, 2, 1)\n",
    "            action_embeddings = action_embeddings.permute(0, 2, 1)\n",
    "\n",
    "        # stack rtg, states and actions and reshape sequence as\n",
    "        # (r1, s1, a1, r2, s2, a2 ...)\n",
    "        h = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
    "\n",
    "        h = self.embed_ln(h)\n",
    "\n",
    "        # transformer and prediction\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
    "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
    "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
    "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
    "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
    "        state_preds = self.predict_state(h[:,2].permute(0, 2, 1))    # predict next state given r, s, a\n",
    "        action_preds = self.predict_action(h[:,1].permute(0, 2, 1))  # predict action given r, s\n",
    "\n",
    "        return state_preds, action_preds, return_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma):\n",
    "    disc_cumsum = np.zeros_like(x)\n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "    return disc_cumsum\n",
    "class D4RLTrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataset_path, context_len, rtg_scale):\n",
    "        self.context_len = context_len\n",
    "\n",
    "        # load dataset\n",
    "        with open(dataset_path, 'rb') as f:\n",
    "            self.trajectories = pickle.load(f)\n",
    "\n",
    "        # Handle Gymnasium `timeouts`\n",
    "        for traj in self.trajectories:\n",
    "            if 'timeouts' in traj:\n",
    "                traj['terminals'] = np.logical_or(traj['terminals'], traj['timeouts'])\n",
    "\n",
    "        # calculate min len of traj, state mean and variance\n",
    "        # and returns_to-go for all traj\n",
    "        min_len = 10**6\n",
    "        states = []\n",
    "        for traj in self.trajectories:\n",
    "            traj_len = traj['observations'].shape[0]\n",
    "            min_len = min(min_len, traj_len)\n",
    "            states.append(traj['observations'])\n",
    "            # calculate returns-to-go and rescale them\n",
    "            traj['returns_to_go'] = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        for traj in self.trajectories:\n",
    "            traj['observations'] = (traj['observations'] - self.state_mean) / self.state_std\n",
    "\n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = traj['observations'].shape[0]\n",
    "\n",
    "        if traj_len >= self.context_len:\n",
    "            # sample random index to slice trajectory\n",
    "            si = random.randint(0, traj_len - self.context_len)\n",
    "\n",
    "            states = torch.from_numpy(traj['observations'][si : si + self.context_len])\n",
    "            actions = torch.from_numpy(traj['actions'][si : si + self.context_len])\n",
    "            returns_to_go = torch.from_numpy(traj['returns_to_go'][si : si + self.context_len])\n",
    "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "\n",
    "            # all ones since no padding\n",
    "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            padding_len = self.context_len - traj_len\n",
    "\n",
    "            # padding with zeros\n",
    "            states = torch.from_numpy(traj['observations'])\n",
    "            states = torch.cat([states,\n",
    "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                dtype=states.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            actions = torch.from_numpy(traj['actions'])\n",
    "            actions = torch.cat([actions,\n",
    "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
    "                                dtype=actions.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            returns_to_go = torch.from_numpy(traj['returns_to_go'])\n",
    "            returns_to_go = torch.cat([returns_to_go,\n",
    "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
    "                                dtype=returns_to_go.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
    "\n",
    "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long),\n",
    "                                   torch.zeros(padding_len, dtype=torch.long)],\n",
    "                                  dim=0)\n",
    "\n",
    "        return  timesteps, states, actions, returns_to_go, traj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_states(states):\n",
    "    \"\"\"Reflect the states (e.g., negate specific dimensions).\"\"\"\n",
    "    reflected_states = torch.clone(states)\n",
    "    reflected_states*= -1  \n",
    "    return reflected_states\n",
    "\n",
    "def reflect_actions(actions):\n",
    "    \"\"\"Reflect the actions similarly to states.\"\"\"\n",
    "    reflected_actions = torch.clone(actions)\n",
    "    reflected_actions*= -1  \n",
    "    return reflected_actions\n",
    "\n",
    "def test_equivariance(model, timesteps, states, actions, returns_to_go, traj_mask):\n",
    "    \"\"\"Test equivariance of the model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Reflect states and actions\n",
    "    reflected_states = reflect_states(states)\n",
    "    reflected_actions = reflect_actions(actions)\n",
    "\n",
    "    # Get predictions for original inputs\n",
    "    with torch.no_grad():\n",
    "        state_preds, action_preds, _ = model.forward(\n",
    "            timesteps=timesteps,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            returns_to_go=returns_to_go\n",
    "        )\n",
    "\n",
    "        # Get predictions for reflected inputs\n",
    "        reflected_state_preds, reflected_action_preds, _ = model.forward(\n",
    "            timesteps=timesteps,\n",
    "            states=reflected_states,\n",
    "            actions=reflected_actions,\n",
    "            returns_to_go=returns_to_go\n",
    "        )\n",
    "\n",
    "    # Reflect the predictions back\n",
    "    reflected_state_preds = reflect_states(reflected_state_preds)\n",
    "    reflected_action_preds = reflect_actions(reflected_action_preds)\n",
    "\n",
    "    # Compute equivariance loss\n",
    "    state_equivariance_loss = torch.mean((state_preds - reflected_state_preds) ** 2).item()\n",
    "    action_equivariance_loss = torch.mean((action_preds - reflected_action_preds) ** 2).item()\n",
    "\n",
    "    return state_equivariance_loss, action_equivariance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"stitched\"       # medium / medium-replay / medium-expert\n",
    "rtg_scale = 1000                # scale to normalize returns to go\n",
    "\n",
    "\n",
    "env_name = 'InvertedPendulum-v4'\n",
    "rtg_target = 10000\n",
    "env_d4rl_name = f'InvertedPendulum-v4-{dataset}'\n",
    "\n",
    "\n",
    "max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "num_eval_ep = 10            # num of evaluation episodes per iteration\n",
    "\n",
    "batch_size = 64             # training batch size\n",
    "lr = 4e-4                   # learning rate\n",
    "wt_decay = 1e-4             # weight decay\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 500\n",
    "num_updates_per_iter = 500\n",
    "\n",
    "context_len = 5        # K in decision transformer\n",
    "n_blocks = 2            # num of transformer blocks\n",
    "embed_dim = 128         # embedding (hidden) dim of transformer\n",
    "n_heads = 1             # num of transformer heads\n",
    "dropout_p = 0.1         # dropout probability\n",
    "\n",
    "\n",
    "\n",
    "# load data from this file\n",
    "dataset_path = f'processed_data/{env_d4rl_name}.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "State Equivariance Loss: 31.83144221114749\n",
      "Action Equivariance Loss: 2.2801432942993947e-24\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "model = DecisionTransformer(\n",
    "    state_dim=state_dim,  # Replace with the appropriate dimension\n",
    "    act_dim=act_dim,      # Replace with the appropriate dimension\n",
    "    n_blocks=n_blocks,    # Number of transformer blocks\n",
    "    h_dim=embed_dim,      # Hidden dimension of embeddings\n",
    "    context_len=context_len,  # Context length\n",
    "    n_heads=n_heads,      # Number of attention heads\n",
    "    drop_p=dropout_p      # Dropout probability\n",
    ").to(device)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "traj_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
    "\n",
    "traj_data_loader = DataLoader(\n",
    "    traj_dataset,\n",
    "    batch_size=1,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Get a batch of test data\n",
    "data_iter = iter(traj_data_loader)\n",
    "timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\n",
    "# Move data to device\n",
    "timesteps = timesteps.to(device)\n",
    "states = states.to(device)\n",
    "actions = actions.to(device)\n",
    "returns_to_go = returns_to_go.to(device).unsqueeze(dim=-1)\n",
    "traj_mask = traj_mask.to(device)\n",
    "\n",
    "# Test equivariance\n",
    "state_eq_loss, action_eq_loss = test_equivariance(\n",
    "    model, timesteps, states, actions, returns_to_go, traj_mask\n",
    ")\n",
    "\n",
    "print(f\"State Equivariance Loss: {state_eq_loss}\")\n",
    "print(f\"Action Equivariance Loss: {action_eq_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Equivariance Loss: 2.8138831360318366\n",
      "Action Equivariance Loss: 1.1683136072647011e-19\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of test data\n",
    "data_iter = iter(traj_data_loader)\n",
    "timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\n",
    "# Move data to device\n",
    "timesteps = timesteps.to(device)\n",
    "states = states.to(device)\n",
    "actions = actions.to(device)\n",
    "returns_to_go = returns_to_go.to(device).unsqueeze(dim=-1)\n",
    "traj_mask = traj_mask.to(device)\n",
    "\n",
    "# Test equivariance\n",
    "state_eq_loss, action_eq_loss = test_equivariance(\n",
    "    model, timesteps, states, actions, returns_to_go, traj_mask\n",
    ")\n",
    "\n",
    "print(f\"State Equivariance Loss: {state_eq_loss}\")\n",
    "print(f\"Action Equivariance Loss: {action_eq_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Euib_RmfOiT3",
    "aBD3fRknjEj6",
    "9TpGEYTblzQc",
    "wNJM0LG1iziA",
    "gLHjV3q28LNr",
    "pewE01Ca4BG0",
    "QXXrs_PjAHrN",
    "wxcJqnb1Him4"
   ],
   "name": "min_decision_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "transformerdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
