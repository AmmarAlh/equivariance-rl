{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2Nk5Gp7hUGA"
   },
   "source": [
    "# import libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "q4xiijmBixUm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from symmetrizer.nn.modules import BasisLinear\n",
    "from symmetrizer.ops import GroupRepresentations\n",
    "from symmetrizer.groups import MatrixRepresentation\n",
    "from utils.symmetrizer_utils import create_inverted_pendulum_actor_representations, create_inverted_pendulum_qfunction_representations, actor_equivariance_mae, q_equivariance_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_emlp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the BasisLinear layer for testing\n",
    "# state_dim = 4  # Dimensionality of the input state\n",
    "# h_dim = 128     # Dimensionality of the output representation\n",
    "# # Test input: batch of states\n",
    "# batch_size = 64  # Example batch size\n",
    "# seq_len = 20    # Sequence length (context length)\n",
    "# in_action_embedding = [\n",
    "# torch.DoubleTensor(np.eye(4)), \n",
    "# torch.DoubleTensor(-1 * np.eye(4))\n",
    "# ]\n",
    "# in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "\n",
    "# out_action_embedding =  [\n",
    "#     torch.DoubleTensor(np.eye(seq_len*2)), \n",
    "#     torch.DoubleTensor(-1 * np.eye(seq_len*2))\n",
    "# ]\n",
    "# out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "# repr_inter = MatrixRepresentation(in_group, out_group)\n",
    "\n",
    "# basis_layer = BasisLinear(\n",
    "#     channels_in=1,  # Input dimensionality\n",
    "#     channels_out=h_dim,     # Output dimensionality\n",
    "#     group=repr_inter,          # Group representation for equivariance\n",
    "# )\n",
    "\n",
    "\n",
    "# # Create the environment\n",
    "# env = gym.make(\"InvertedPendulum-v4\")\n",
    "\n",
    "# # Reset the environment and get the initial state\n",
    "# x, _ = env.reset()  # Unpack observation and metadata\n",
    "\n",
    "# # Convert the observation to a PyTorch tensor and add a batch dimension\n",
    "# t_x = torch.Tensor(x).unsqueeze(0).unsqueeze(0)  # Shape [1, state_dim]\n",
    "# dummy_input = torch.randn(batch_size, seq_len ,state_dim)\n",
    "# print(\"Initial observation:\", dummy_input.shape)\n",
    "\n",
    "# layer = nn.Linear(state_dim, h_dim)\n",
    "# #output = basis_layer(dummy_input).permute(0, 2, 1) \n",
    "# output = layer(dummy_input)\n",
    "\n",
    "# print(\"Output shape:\", output.shape)      \n",
    "# # proint the first 10 elements of the output\n",
    "# # Print the first 10 elements of the output\n",
    "# print(\"Output:\", output[1, 1, : ].shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNJM0LG1iziA"
   },
   "source": [
    "# decision transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHMl_Y1SicXb"
   },
   "outputs": [],
   "source": [
    "class MaskedCausalAttention(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p, context_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.max_T = max_T\n",
    "         ## group representations\n",
    "        in_action_embedding = [\n",
    "        torch.DoubleTensor(np.eye(context_len*3)), \n",
    "        torch.DoubleTensor(-1 * np.eye(context_len*3))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "        \n",
    "        \n",
    "\n",
    "        out_action_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len*3)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len*3))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "\n",
    "        repr_inter = MatrixRepresentation(in_group, out_group)\n",
    "\n",
    "        if use_emlp:\n",
    "            self.q_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.k_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.v_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "        else:\n",
    "            self.q_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.k_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "            self.v_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "\n",
    "        self.proj_net = nn.Linear(h_dim, h_dim, dtype=torch.float64)\n",
    "\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "        ones = torch.ones((max_T, max_T))\n",
    "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
    "\n",
    "        # register buffer makes sure mask does not get updated\n",
    "        # during backpropagation\n",
    "        self.register_buffer('mask',mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
    "\n",
    "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
    "\n",
    "        # rearrange q, k, v as (B, N, T, D)\n",
    "        if use_emlp:\n",
    "            q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
    "            k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
    "            v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        else:\n",
    "\n",
    "            q = self.q_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "            k = self.k_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "            v = self.v_net(x.permute(0,2,1)).view(B, T, N, D).transpose(1,2)\n",
    "\n",
    "        # weights (B, N, T, T)\n",
    "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
    "        # causal mask applied to weights\n",
    "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
    "        # normalize weights, all -inf -> 0 after softmax\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # attention (B, N, T, D)\n",
    "        attention = self.att_drop(normalized_weights @ v)\n",
    "\n",
    "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
    "\n",
    "        out = self.proj_drop(self.proj_net(attention))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p, context_len):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p, context_len)\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(h_dim, 4*h_dim, \n",
    "            dtype=torch.float64),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4*h_dim, h_dim,\n",
    "            dtype=torch.float64),\n",
    "                nn.Dropout(drop_p))\n",
    "        self.ln1 = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "        self.ln2 = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
    "        x = x + self.attention(x) # residual\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x) # residual\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
    "                 n_heads, drop_p, max_timestep=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.h_dim = h_dim\n",
    "        \n",
    "        ## group representations\n",
    "        in_action_embedding = [\n",
    "            torch.DoubleTensor(np.eye(act_dim)), \n",
    "            torch.DoubleTensor(-1 * np.eye(act_dim))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_action_embedding, \"ActionRepr\")\n",
    "        \n",
    "        \n",
    "\n",
    "        out_action_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_action_embedding, \"ActionRepr\")\n",
    "\n",
    "\n",
    "        repr_in = MatrixRepresentation(in_group, out_group)\n",
    "        repr_out = MatrixRepresentation(out_group, in_group)\n",
    "        ########################################################\n",
    "        in_state_embedding = [\n",
    "        torch.DoubleTensor(np.eye(state_dim)), \n",
    "        torch.DoubleTensor(-1 * np.eye(state_dim))\n",
    "        ]\n",
    "        in_group = GroupRepresentations(in_state_embedding, \"ActionRepr\")\n",
    "        \n",
    "\n",
    "        out_state_embedding =  [\n",
    "            torch.DoubleTensor(np.eye(context_len)), \n",
    "            torch.DoubleTensor(-1 * np.eye(context_len))\n",
    "        ]\n",
    "        out_group = GroupRepresentations(out_state_embedding, \"ActionRepr\")\n",
    "        \n",
    "        repr_in_s = MatrixRepresentation(in_group, out_group)\n",
    "        repr_out_s = MatrixRepresentation(out_group, in_group)\n",
    "        ### transformer blocks\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p, context_len) for _ in range(n_blocks)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "\n",
    "        ### projection heads (project to embedding)\n",
    "        self.embed_ln = nn.LayerNorm(h_dim, dtype=torch.float64)\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
    "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
    "        if use_emlp:\n",
    "            self.embed_state = BasisLinear(1, h_dim, group=repr_in_s)\n",
    "        else:\n",
    "            self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
    "\n",
    "        # # discrete actions\n",
    "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
    "        # use_action_tanh = False # False for discrete actions\n",
    "\n",
    "        # continuous actions\n",
    "        if use_emlp:\n",
    "            self.embed_action = BasisLinear(1, h_dim, group=repr_in)\n",
    "            use_action_tanh = True # True for continuous actions\n",
    "        else:\n",
    "            self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
    "            use_action_tanh = False\n",
    "\n",
    "        ### prediction heads\n",
    "        self.predict_rtg = torch.nn.Linear(h_dim, 1, dtype=torch.float64)\n",
    "        \n",
    "        if use_emlp:\n",
    "            self.predict_state = BasisLinear(h_dim, 1, group=repr_out_s)\n",
    "            self.predict_action = nn.Sequential(\n",
    "                *([BasisLinear(h_dim, 1, group=repr_out)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "            )\n",
    "        else:\n",
    "            self.predict_state = torch.nn.Linear(h_dim, state_dim, dtype=torch.float64)\n",
    "            self.predict_action = nn.Sequential(\n",
    "                *([nn.Linear(h_dim, act_dim, dtype=torch.float64)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, timesteps, states, actions, returns_to_go):\n",
    "\n",
    "        B, T, _ = states.shape\n",
    "\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = self.embed_state(states) #+ time_embeddings\n",
    "        action_embeddings = self.embed_action(actions)# + time_embeddings\n",
    "        returns_embeddings = self.embed_rtg(returns_to_go) #+ time_embeddings\n",
    "        \n",
    "        if use_emlp:\n",
    "            state_embeddings = state_embeddings.permute(0, 2, 1)\n",
    "            action_embeddings = action_embeddings.permute(0, 2, 1)\n",
    "\n",
    "        # stack rtg, states and actions and reshape sequence as\n",
    "        # (r1, s1, a1, r2, s2, a2 ...)\n",
    "        h = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
    "\n",
    "        h = self.embed_ln(h)\n",
    "\n",
    "        # transformer and prediction\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
    "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
    "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
    "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
    "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
    "        state_preds = self.predict_state(h[:,2].permute(0, 2, 1))    # predict next state given r, s, a\n",
    "        action_preds = self.predict_action(h[:,1].permute(0, 2, 1))  # predict action given r, s\n",
    "\n",
    "        return state_preds, action_preds, return_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma):\n",
    "    disc_cumsum = np.zeros_like(x)\n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "    return disc_cumsum\n",
    "class D4RLTrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataset_path, context_len, rtg_scale):\n",
    "        self.context_len = context_len\n",
    "\n",
    "        # load dataset\n",
    "        with open(dataset_path, 'rb') as f:\n",
    "            self.trajectories = pickle.load(f)\n",
    "\n",
    "        # Handle Gymnasium `timeouts`\n",
    "        for traj in self.trajectories:\n",
    "            if 'timeouts' in traj:\n",
    "                traj['terminals'] = np.logical_or(traj['terminals'], traj['timeouts'])\n",
    "\n",
    "        # calculate min len of traj, state mean and variance\n",
    "        # and returns_to-go for all traj\n",
    "        min_len = 10**6\n",
    "        states = []\n",
    "        for traj in self.trajectories:\n",
    "            traj_len = traj['observations'].shape[0]\n",
    "            min_len = min(min_len, traj_len)\n",
    "            states.append(traj['observations'])\n",
    "            # calculate returns-to-go and rescale them\n",
    "            traj['returns_to_go'] = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
    "\n",
    "        # used for input normalization\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "        # normalize states\n",
    "        for traj in self.trajectories:\n",
    "            traj['observations'] = (traj['observations'] - self.state_mean) / self.state_std\n",
    "\n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "        traj_len = traj['observations'].shape[0]\n",
    "\n",
    "        if traj_len >= self.context_len:\n",
    "            # sample random index to slice trajectory\n",
    "            si = random.randint(0, traj_len - self.context_len)\n",
    "\n",
    "            states = torch.from_numpy(traj['observations'][si : si + self.context_len])\n",
    "            actions = torch.from_numpy(traj['actions'][si : si + self.context_len])\n",
    "            returns_to_go = torch.from_numpy(traj['returns_to_go'][si : si + self.context_len])\n",
    "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
    "\n",
    "            # all ones since no padding\n",
    "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            padding_len = self.context_len - traj_len\n",
    "\n",
    "            # padding with zeros\n",
    "            states = torch.from_numpy(traj['observations'])\n",
    "            states = torch.cat([states,\n",
    "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
    "                                dtype=states.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            actions = torch.from_numpy(traj['actions'])\n",
    "            actions = torch.cat([actions,\n",
    "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
    "                                dtype=actions.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            returns_to_go = torch.from_numpy(traj['returns_to_go'])\n",
    "            returns_to_go = torch.cat([returns_to_go,\n",
    "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
    "                                dtype=returns_to_go.dtype)],\n",
    "                               dim=0)\n",
    "\n",
    "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
    "\n",
    "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long),\n",
    "                                   torch.zeros(padding_len, dtype=torch.long)],\n",
    "                                  dim=0)\n",
    "\n",
    "        return  timesteps, states, actions, returns_to_go, traj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_states(states):\n",
    "    \"\"\"Reflect the states (e.g., negate specific dimensions).\"\"\"\n",
    "    reflected_states = torch.clone(states)\n",
    "    reflected_states*= -1  \n",
    "    return reflected_states\n",
    "\n",
    "def reflect_actions(actions):\n",
    "    \"\"\"Reflect the actions similarly to states.\"\"\"\n",
    "    reflected_actions = torch.clone(actions)\n",
    "    reflected_actions*= -1  \n",
    "    return reflected_actions\n",
    "\n",
    "def test_equivariance(model, timesteps, states, actions, returns_to_go, traj_mask):\n",
    "    \"\"\"Test equivariance of the model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Reflect states and actions\n",
    "    reflected_states = reflect_states(states)\n",
    "    reflected_actions = reflect_actions(actions)\n",
    "\n",
    "    # Get predictions for original inputs\n",
    "    with torch.no_grad():\n",
    "        state_preds, action_preds, _ = model.forward(\n",
    "            timesteps=timesteps,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            returns_to_go=returns_to_go\n",
    "        )\n",
    "\n",
    "        # Get predictions for reflected inputs\n",
    "        reflected_state_preds, reflected_action_preds, _ = model.forward(\n",
    "            timesteps=timesteps,\n",
    "            states=reflected_states,\n",
    "            actions=reflected_actions,\n",
    "            returns_to_go=returns_to_go\n",
    "        )\n",
    "\n",
    "    # Reflect the predictions back\n",
    "    reflected_state_preds = reflect_states(reflected_state_preds)\n",
    "    reflected_action_preds = reflect_actions(reflected_action_preds)\n",
    "\n",
    "    # Compute equivariance loss\n",
    "    state_equivariance_loss = torch.mean((state_preds - reflected_state_preds) ** 2).item()\n",
    "    action_equivariance_loss = torch.mean((action_preds - reflected_action_preds) ** 2).item()\n",
    "\n",
    "    return state_equivariance_loss, action_equivariance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"stitched\"       # medium / medium-replay / medium-expert\n",
    "rtg_scale = 1000                # scale to normalize returns to go\n",
    "\n",
    "\n",
    "env_name = 'InvertedPendulum-v4'\n",
    "rtg_target = 10000\n",
    "env_d4rl_name = f'InvertedPendulum-v4-{dataset}'\n",
    "\n",
    "\n",
    "max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "num_eval_ep = 10            # num of evaluation episodes per iteration\n",
    "\n",
    "batch_size = 64             # training batch size\n",
    "lr = 4e-4                   # learning rate\n",
    "wt_decay = 1e-4             # weight decay\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 500\n",
    "num_updates_per_iter = 500\n",
    "\n",
    "context_len = 5        # K in decision transformer\n",
    "n_blocks = 2            # num of transformer blocks\n",
    "embed_dim = 128         # embedding (hidden) dim of transformer\n",
    "n_heads = 1             # num of transformer heads\n",
    "dropout_p = 0.1         # dropout probability\n",
    "\n",
    "\n",
    "\n",
    "# load data from this file\n",
    "dataset_path = f'processed_data/{env_d4rl_name}.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x15 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m traj_mask \u001b[38;5;241m=\u001b[39m traj_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Test equivariance\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m state_eq_loss, action_eq_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_equivariance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns_to_go\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraj_mask\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState Equivariance Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_eq_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction Equivariance Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_eq_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 23\u001b[0m, in \u001b[0;36mtest_equivariance\u001b[0;34m(model, timesteps, states, actions, returns_to_go, traj_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get predictions for original inputs\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     state_preds, action_preds, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns_to_go\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturns_to_go\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Get predictions for reflected inputs\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     reflected_state_preds, reflected_action_preds, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m     32\u001b[0m         timesteps\u001b[38;5;241m=\u001b[39mtimesteps,\n\u001b[1;32m     33\u001b[0m         states\u001b[38;5;241m=\u001b[39mreflected_states,\n\u001b[1;32m     34\u001b[0m         actions\u001b[38;5;241m=\u001b[39mreflected_actions,\n\u001b[1;32m     35\u001b[0m         returns_to_go\u001b[38;5;241m=\u001b[39mreturns_to_go\n\u001b[1;32m     36\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[52], line 209\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, timesteps, states, actions, returns_to_go)\u001b[0m\n\u001b[1;32m    206\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_ln(h)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# transformer and prediction\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# get h reshaped such that its size = (B x 3 x T x h_dim) and\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\u001b[39;00m\n\u001b[1;32m    215\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mreshape(B, T, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[52], line 94\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Attention -> LayerNorm -> MLP -> LayerNorm\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# residual\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[1;32m     96\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x) \u001b[38;5;66;03m# residual\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[52], line 53\u001b[0m, in \u001b[0;36mMaskedCausalAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# rearrange q, k, v as (B, N, T, D)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_emlp:\n\u001b[0;32m---> 53\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(B, T, N, D)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_net(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(B, T, N, D)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_net(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(B, T, N, D)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/transformerdt/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x15 and 128x128)"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(env_name)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "model = DecisionTransformer(\n",
    "    state_dim=state_dim,  # Replace with the appropriate dimension\n",
    "    act_dim=act_dim,      # Replace with the appropriate dimension\n",
    "    n_blocks=n_blocks,    # Number of transformer blocks\n",
    "    h_dim=embed_dim,      # Hidden dimension of embeddings\n",
    "    context_len=context_len,  # Context length\n",
    "    n_heads=n_heads,      # Number of attention heads\n",
    "    drop_p=dropout_p      # Dropout probability\n",
    ").to(device)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "traj_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
    "\n",
    "traj_data_loader = DataLoader(\n",
    "    traj_dataset,\n",
    "    batch_size=1,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Get a batch of test data\n",
    "data_iter = iter(traj_data_loader)\n",
    "timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\n",
    "# Move data to device\n",
    "timesteps = timesteps.to(device)\n",
    "states = states.to(device)\n",
    "actions = actions.to(device)\n",
    "returns_to_go = returns_to_go.to(device).unsqueeze(dim=-1)\n",
    "traj_mask = traj_mask.to(device)\n",
    "\n",
    "# Test equivariance\n",
    "state_eq_loss, action_eq_loss = test_equivariance(\n",
    "    model, timesteps, states, actions, returns_to_go, traj_mask\n",
    ")\n",
    "\n",
    "print(f\"State Equivariance Loss: {state_eq_loss}\")\n",
    "print(f\"Action Equivariance Loss: {action_eq_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Equivariance Loss: 2.5476254518182375\n",
      "Action Equivariance Loss: 1.061262476728831e-24\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of test data\n",
    "data_iter = iter(traj_data_loader)\n",
    "timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\n",
    "# Move data to device\n",
    "timesteps = timesteps.to(device)\n",
    "states = states.to(device)\n",
    "actions = actions.to(device)\n",
    "returns_to_go = returns_to_go.to(device).unsqueeze(dim=-1)\n",
    "traj_mask = traj_mask.to(device)\n",
    "\n",
    "# Test equivariance\n",
    "state_eq_loss, action_eq_loss = test_equivariance(\n",
    "    model, timesteps, states, actions, returns_to_go, traj_mask\n",
    ")\n",
    "\n",
    "print(f\"State Equivariance Loss: {state_eq_loss}\")\n",
    "print(f\"Action Equivariance Loss: {action_eq_loss}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Euib_RmfOiT3",
    "aBD3fRknjEj6",
    "9TpGEYTblzQc",
    "wNJM0LG1iziA",
    "gLHjV3q28LNr",
    "pewE01Ca4BG0",
    "QXXrs_PjAHrN",
    "wxcJqnb1Him4"
   ],
   "name": "min_decision_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "transformerdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
