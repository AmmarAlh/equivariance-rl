{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qspqbJOVOpN-"
      },
      "source": [
        "## Equivaraint TD3 Actor and QNetwork Test (JAX Implementation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCUFxefFNgfT",
        "outputId": "f5da9bd2-74eb-46ad-8a91-6c445d15a0c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "%pip install -q  gymnasium[mujoco] jax jaxlib flax optax tyro stable-baselines3 torch tensorboard emlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b0g6BMztPLnM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ammaral/Projects/equivaraince-rl/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
          ]
        }
      ],
      "source": [
        "# Importing the nessesary packages for the entire code\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import gymnasium as gym\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from emlp.groups import SO, C, D, Trivial\n",
        "from emlp.nn.flax import EMLPBlock, Linear, Sequential, uniform_rep\n",
        "from emlp.reps import Scalar, Vector, Rep\n",
        "from typing import Callable\n",
        "import torch\n",
        "from flax.training.train_state import TrainState\n",
        "import optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOI6ZRJYQdiQ"
      },
      "source": [
        "## The standard networks and their equivaraint version using the emlp package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WAAvBhj-Pq-b"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    ch: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray, a: jnp.ndarray):\n",
        "        x = jnp.concatenate([x, a], -1)\n",
        "        x = nn.Dense(self.ch)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.ch)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(1)(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InvariantQNetwork(nn.Module):\n",
        "    rep_in: Callable\n",
        "    rep_out: Callable\n",
        "    group: Callable\n",
        "    ch: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, a):\n",
        "        rep_in = self.rep_in(self.group)\n",
        "        rep_out = self.rep_out(self.group)\n",
        "        middle_layers = uniform_rep(self.ch, self.group)\n",
        "        x = jnp.concatenate([x, a], axis=1)\n",
        "        network = Sequential(\n",
        "            EMLPBlock(rep_in=rep_in, rep_out=middle_layers),\n",
        "            EMLPBlock(rep_in=middle_layers, rep_out=middle_layers),\n",
        "            Linear(middle_layers, rep_out),\n",
        "        )\n",
        "\n",
        "        return network(x)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    action_dim: int\n",
        "    action_scale: jnp.ndarray\n",
        "    action_bias: jnp.ndarray\n",
        "    ch: int = 256\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.ch)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.ch)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.action_dim)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = x * self.action_scale + self.action_bias\n",
        "        return x\n",
        "\n",
        "\n",
        "class EquiActor(nn.Module):\n",
        "    action_dim: int\n",
        "    action_scale: jnp.ndarray\n",
        "    action_bias: jnp.ndarray\n",
        "    rep_in: Callable\n",
        "    rep_out: Callable\n",
        "    group: Callable\n",
        "    ch: int = 128\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        rep_in = self.rep_in(self.group)\n",
        "        rep_out = self.rep_out(self.group)\n",
        "        middle_layers = uniform_rep(self.ch, self.group)\n",
        "\n",
        "        fc_mu = Sequential(\n",
        "            EMLPBlock(rep_in=rep_in, rep_out=middle_layers),\n",
        "            EMLPBlock(rep_in=middle_layers, rep_out=middle_layers),\n",
        "            Linear(middle_layers, rep_out),\n",
        "        )\n",
        "\n",
        "        x = jax.nn.tanh(fc_mu(x))\n",
        "        x = x * self.action_scale + self.action_bias\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IpbxZd_YZUZ"
      },
      "source": [
        "## Representation for reflection across the vecrtical axis for the action in inverted pendulum enviroment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DB8mF8pCX3Uc"
      },
      "outputs": [],
      "source": [
        "class InvertedPendulumActionRep(Rep):\n",
        "    \"\"\"Representation for reflection across the vecrtical axis for the action in inverted pendulum enviroment.\"\"\"\n",
        "\n",
        "    def __init__(self, G):\n",
        "        self.G = G  # The group to which this representation is associated\n",
        "        self.is_permutation = True\n",
        "        super().__init__()\n",
        "    def rho(self, M):\n",
        "        \"\"\"\n",
        "        Group representation of the matrix M.\n",
        "        M should be either the identity or reflection matrix.\n",
        "        \"\"\"\n",
        "        if jnp.allclose(M, jnp.eye(2)):\n",
        "            return jnp.eye(1)  # Identity matrix, no change\n",
        "        elif jnp.allclose(M, jnp.array([[-1, 0], [0, -1]])):\n",
        "            return -1*jnp.eye(1)   # Sign flip for action\n",
        "        else:\n",
        "            raise ValueError(\"Unrecognized group element\")\n",
        "\n",
        "    def size(self):\n",
        "        assert self.G is not None, f\"must know G to find size for rep={self}\"\n",
        "        return 1\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"InvertedPendulumActionRep\"\n",
        "    def __call__(self,G):\n",
        "        return self.__class__(G)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9X5-kIaXJ_l"
      },
      "source": [
        "## Testing Equivaraince Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jxJIr_qNXVEQ"
      },
      "outputs": [],
      "source": [
        "G = C(2)\n",
        "env_id = \"InvertedPendulum-v4\"\n",
        "# Create the state and action representations\n",
        "envs = gym.make(env_id)\n",
        "envs.observation_space.dtype = np.float64\n",
        "\n",
        "obs,_ = envs.reset()\n",
        "key = jax.random.PRNGKey(1)\n",
        "\n",
        "class TrainState(TrainState):\n",
        "    target_params: flax.core.FrozenDict\n",
        "\n",
        "repin_actor = Vector(G) + Vector(G)\n",
        "repout_actor = InvertedPendulumActionRep(G)\n",
        "\n",
        "repin_q = Vector(G) + Vector(G) + InvertedPendulumActionRep(G)\n",
        "repout_q = Scalar(G)\n",
        "\n",
        "actor = EquiActor(\n",
        "    action_dim=np.prod(envs.action_space),\n",
        "    action_scale=jnp.array(\n",
        "        (envs.action_space.high - envs.action_space.low) / 2.0\n",
        "    ),\n",
        "    action_bias=jnp.array(\n",
        "        (envs.action_space.high + envs.action_space.low) / 2.0\n",
        "    ),\n",
        "    rep_in=repin_actor,\n",
        "    rep_out=repout_actor,\n",
        "    group=G,\n",
        "    ch=128,\n",
        ")\n",
        "qf = InvariantQNetwork(rep_in=repin_q, rep_out=repout_q, group=G, ch=128)\n",
        "\n",
        "key, actor_key, expert_actor_key, qf_key = jax.random.split(key, 4)\n",
        "actor_state = TrainState.create(\n",
        "    apply_fn=actor.apply,\n",
        "    params=actor.init(actor_key, obs),\n",
        "    target_params=actor.init(actor_key, obs),\n",
        "    tx=optax.adam(learning_rate=1e-3),\n",
        ")\n",
        "\n",
        "qf_state = TrainState.create(\n",
        "    apply_fn=qf.apply,\n",
        "    params=qf.init(qf_key, obs.reshape(1,-1), envs.action_space.sample().reshape(1,-1)),\n",
        "    target_params=qf.init(qf_key, obs.reshape(1,-1), envs.action_space.sample().reshape(1,-1)),\n",
        "    tx=optax.adam(learning_rate=1e-3),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HtsauCxQZKXH"
      },
      "outputs": [],
      "source": [
        "def rel_err(a, b):\n",
        "    return np.array(\n",
        "        jnp.sqrt(((a - b) ** 2).mean())\n",
        "        / (jnp.sqrt((a**2).mean()) + jnp.sqrt((b**2).mean()))\n",
        "    )\n",
        "\n",
        "# equivaraince error function for the actor network\n",
        "def equivariance_err_actor(model, params, state, rin, rout, G):\n",
        "    gs = G.samples(5)\n",
        "    rho_gin = jnp.stack([jnp.array(rin.rho_dense(g)) for g in gs])\n",
        "    rho_gout = jnp.stack([jnp.array(rout.rho_dense(g)) for g in gs])\n",
        "    y1 = model.apply(params, (rho_gin @ state[..., None]).squeeze(-1))\n",
        "    y2 = model.apply(params, state)\n",
        "    y2 = (rho_gout @ y2[..., None]).squeeze(-1)\n",
        "    error = rel_err(y1, y2)\n",
        "    print(\"Equivariance error:\", error)\n",
        "    return error\n",
        "\n",
        "\n",
        "# equivaraince error function for Q Network\n",
        "def equivariance_err_qvalue(model, params, state, actions, rin, rout, G):\n",
        "    gs = G.samples(5)\n",
        "    rho_gin = jnp.stack([jnp.array(rin.rho_dense(g)) for g in gs])\n",
        "    rho_gout = jnp.stack([jnp.array(rout.rho_dense(g)) for g in gs])\n",
        "    x = jnp.concatenate([state, actions], axis=1)\n",
        "    x = (rho_gin @ x[..., None]).squeeze(-1)\n",
        "    y1 = model.apply(params, x[:, :state.shape[-1]], x[:,-actions.shape[-1]:])\n",
        "    y2 = model.apply(params, state, actions)\n",
        "    y2 = (rho_gout @ y2[..., None]).squeeze(-1)\n",
        "    error = rel_err(y1, y2)\n",
        "    print(\"Equivariance error:\", error)\n",
        "    return error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aSHpjNfZGFY",
        "outputId": "eda5e3cd-5a30-4bb1-d615-725f0ae1010c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equivariance error: 5.0193375e-08\n",
            "\n",
            "Equivariance error: 0.38149637\n"
          ]
        }
      ],
      "source": [
        "equiv_error =  equivariance_err_actor(\n",
        "                        actor, actor_state.params, obs.reshape(1,-1), repin_actor, repout_actor, G\n",
        "                    )\n",
        "print()\n",
        "action = actor.apply(actor_state.params, obs)\n",
        "equiv_error =  equivariance_err_qvalue(\n",
        "                        qf, qf_state.params, obs.reshape(1,-1), action.reshape(1,-1),repin_q, repout_q, G\n",
        "                    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
